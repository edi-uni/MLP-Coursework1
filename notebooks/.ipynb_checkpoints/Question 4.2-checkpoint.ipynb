{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAIN AND VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "def train_model_and_plot_stats(\n",
    "        model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval, fig_name, notebook=True):\n",
    "    \n",
    "    # As well as monitoring the error over training also monitor classification\n",
    "    # accuracy i.e. proportion of most-probable predicted classes being equal to targets\n",
    "    data_monitors={'acc': lambda y, t: (y.argmax(-1) == t.argmax(-1)).mean()}\n",
    "\n",
    "    # Use the created objects to initialise a new Optimiser instance.\n",
    "    optimiser = Optimiser(\n",
    "        model, error, learning_rule, train_data, valid_data, None, data_monitors, notebook=notebook)\n",
    "\n",
    "    # Run the optimiser for 5 epochs (full passes through the training set)\n",
    "    # printing statistics every epoch.\n",
    "    stats, keys, run_time = optimiser.train(num_epochs=num_epochs, stats_interval=stats_interval)\n",
    "\n",
    "    # Plot the change in the validation and training set error over training.\n",
    "    fig_1 = plt.figure(figsize=(8, 4))\n",
    "    ax_1 = fig_1.add_subplot(111)\n",
    "    for k in ['error(train)', 'error(valid)']:\n",
    "        ax_1.plot(np.arange(1, stats.shape[0]) * stats_interval, \n",
    "                  stats[1:, keys[k]], label=k)\n",
    "#     ax_1.legend(loc=0)\n",
    "    ax_1.grid('on') # Turn axes grid on\n",
    "    ax_1.legend(loc='best', fontsize=11) # Add a legend\n",
    "    ax_1.set_xlabel('Epoch number')\n",
    "\n",
    "    fig_1.tight_layout() # This minimises whitespace around the axes.\n",
    "    fig_1.savefig('err_' + fig_name) # Save figure to current directory in PDF format\n",
    "    \n",
    "    # Plot the change in the validation and training set accuracy over training.\n",
    "    fig_2 = plt.figure(figsize=(8, 4))\n",
    "    ax_2 = fig_2.add_subplot(111)\n",
    "    for k in ['acc(train)', 'acc(valid)']:\n",
    "        ax_2.plot(np.arange(1, stats.shape[0]) * stats_interval, \n",
    "                  stats[1:, keys[k]], label=k)\n",
    "#     ax_2.legend(loc=0)\n",
    "    ax_2.grid('on') # Turn axes grid on\n",
    "    ax_2.legend(loc='best', fontsize=11) # Add a legend\n",
    "    ax_2.set_xlabel('Epoch number')\n",
    "    \n",
    "    fig_2.tight_layout() # This minimises whitespace around the axes.\n",
    "    fig_2.savefig('acc_' + fig_name) # Save figure to current directory in PDF format\n",
    "    \n",
    "    return stats, keys, run_time, fig_1, ax_1, fig_2, ax_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAIN AND TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "def test_model(model, error, learning_rule, train_data, test_data, num_epochs, stats_interval, notebook=True):\n",
    "    \n",
    "    # As well as monitoring the error over training also monitor classification\n",
    "    # accuracy i.e. proportion of most-probable predicted classes being equal to targets\n",
    "    data_monitors={'acc': lambda y, t: (y.argmax(-1) == t.argmax(-1)).mean()}\n",
    "\n",
    "    # Use the created objects to initialise a new Optimiser instance.\n",
    "    optimiser = Optimiser(\n",
    "        model, error, learning_rule, train_data, test_data, None,data_monitors, notebook=notebook)\n",
    "\n",
    "    # Run the optimiser for 5 epochs (full passes through the training set)\n",
    "    # printing statistics every epoch.\n",
    "    stats, keys, run_time = optimiser.train(num_epochs=num_epochs, stats_interval=stats_interval)\n",
    "    \n",
    "    return stats, keys, run_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The below code will set up the data providers, random number\n",
    "# generator and logger objects needed for training runs. As\n",
    "# loading the data from file take a little while you generally\n",
    "# will probably not want to reload the data providers on\n",
    "# every training run. If you wish to reset their state you\n",
    "# should instead use the .reset() method of the data providers.\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "import logging\n",
    "from mlp.data_providers import MNISTDataProvider, EMNISTDataProvider\n",
    "\n",
    "# Seed a random number generator\n",
    "seed = 11102018 \n",
    "rng = np.random.RandomState(seed)\n",
    "# Set up a logger object to print info about the training run to stdout\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.handlers = [logging.StreamHandler()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model set up code below is provided as a starting point.\n",
    "# You will probably want to add further code cells for the\n",
    "# different experiments you run.\n",
    "\n",
    "from mlp.layers import AffineLayer, SoftmaxLayer, SigmoidLayer, ReluLayer, LeakyReluLayer\n",
    "from mlp.errors import CrossEntropySoftmaxError\n",
    "from mlp.models import MultipleLayerModel\n",
    "from mlp.initialisers import ConstantInit, GlorotUniformInit\n",
    "from mlp.learning_rules import AdamLearningRule, RMSPropLearningRule, GradientDescentLearningRule, AdaGradLearningRule, AdamLearningRuleWithWeightDecay\n",
    "from mlp.optimisers import Optimiser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set batch size\n",
    "batch_size = 100\n",
    "\n",
    "# Create data provider objects for the MNIST data set\n",
    "train_data = EMNISTDataProvider('train', batch_size=batch_size, rng=rng)\n",
    "valid_data = EMNISTDataProvider('valid', batch_size=batch_size, rng=rng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup hyperparameters\n",
    "num_epochs = 100\n",
    "stats_interval = 1\n",
    "input_dim, output_dim, hidden_dim = 784, 47, 100\n",
    "\n",
    "learning_rates = [0.000001, 0.0000015, 0.00001, 0.000015, 0.0001, 0.00015]  # scale for random parameter initialisation\n",
    "decay_rates = np.linspace(0.1, 0.9, 5)\n",
    "final_errors_train = []\n",
    "final_errors_valid = []\n",
    "final_accs_train = []\n",
    "final_accs_valid = []\n",
    "\n",
    "for i, element in enumerate(product(learning_rates, decay_rates)):\n",
    "    \n",
    "    print('-' * 80)\n",
    "    print('learning_rate={0:.2e}, decay_rate={1:.2e}'\n",
    "          .format(element[0], element[1]))\n",
    "    print('-' * 80)\n",
    "    \n",
    "    # Reset random number generator and data provider states on each run\n",
    "    # to ensure reproducibility of results\n",
    "    rng.seed(seed)\n",
    "    train_data.reset()\n",
    "    valid_data.reset()\n",
    "\n",
    "    weights_init = GlorotUniformInit(rng=rng)\n",
    "    biases_init = ConstantInit(0.)\n",
    "\n",
    "    # Create a model with three hidden layers\n",
    "    model = MultipleLayerModel([\n",
    "        AffineLayer(input_dim, hidden_dim, weights_init, biases_init), \n",
    "        ReluLayer(),\n",
    "        AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "        ReluLayer(),\n",
    "        AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "        ReluLayer(),\n",
    "        AffineLayer(hidden_dim, output_dim, weights_init, biases_init)\n",
    "    ])\n",
    "\n",
    "    # Initialise a cross entropy error object\n",
    "    error = CrossEntropySoftmaxError()\n",
    "\n",
    "    # Use a basic gradient descent learning rule\n",
    "    learning_rule = RMSPropLearningRule(learning_rate=element[0], beta=element[1])\n",
    "\n",
    "    # Remember to use notebook=False when you write a script to be run in a terminal\n",
    "    stats, keys, run_time, fig_1, ax_1, fig_2, ax_2 = train_model_and_plot_stats(\n",
    "        model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval, fig_name='RMSprop_' + str(i) + '.pdf', notebook=True)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    print('    final error(train) = {0:.2e}'.format(stats[-1, keys['error(train)']]))\n",
    "    print('    final error(valid) = {0:.2e}'.format(stats[-1, keys['error(valid)']]))\n",
    "    print('    final acc(train)   = {0:.2e}'.format(stats[-1, keys['acc(train)']]))\n",
    "    print('    final acc(valid)   = {0:.2e}'.format(stats[-1, keys['acc(valid)']]))\n",
    "    print('    run time per epoch = {0:.2f}'.format(run_time * 1. / num_epochs))\n",
    "\n",
    "    final_errors_train.append(stats[-1, keys['error(train)']])\n",
    "    final_errors_valid.append(stats[-1, keys['error(valid)']])\n",
    "    final_accs_train.append(stats[-1, keys['acc(train)']])\n",
    "    final_accs_valid.append(stats[-1, keys['acc(valid)']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 0\n",
    "print('| batch_size | learning_rate | decay_rate | final error(train) | final error(valid) | final acc(train) | final acc(valid) |')\n",
    "print('|------------|---------------|------------|--------------------|--------------------|------------------|------------------|')\n",
    "for element in product(learning_rates, decay_rates):\n",
    "    print('| {0}   | {1:2f}   | {2:.2e}   | {3:.2e}   | {4:.2e}   | {5:.2e}   |  {6:.2f}      | {7:.2f}       |'\n",
    "          .format(j, batch_size, element[0], element[1], \n",
    "                  final_errors_train[j], final_errors_valid[j],\n",
    "                  final_accs_train[j], final_accs_valid[j]))\n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup hyperparameters\n",
    "num_epochs = 100\n",
    "stats_interval = 1\n",
    "input_dim, output_dim, hidden_dim = 784, 47, 100\n",
    "\n",
    "learning_rates = [0.000001, 0.0000015, 0.00001, 0.000015]  # scale for random parameter initialisation\n",
    "decay_rates_1 = [0.85, 0.9, 0.95]\n",
    "decay_rates_2 = [0.95, 0.975, 0.999]\n",
    "final_errors_train = []\n",
    "final_errors_valid = []\n",
    "final_accs_train = []\n",
    "final_accs_valid = []\n",
    "\n",
    "for i, element in enumerate(product(learning_rates, decay_rates_1, decay_rates_2)):\n",
    "    \n",
    "    print('-' * 80)\n",
    "    print('learning_rate={0:.2e}, decay_rate={1:.2e}'\n",
    "          .format(element[0], element[1]))\n",
    "    print('-' * 80)\n",
    "    \n",
    "    # Reset random number generator and data provider states on each run\n",
    "    # to ensure reproducibility of results\n",
    "    rng.seed(seed)\n",
    "    train_data.reset()\n",
    "    valid_data.reset()\n",
    "\n",
    "    weights_init = GlorotUniformInit(rng=rng)\n",
    "    biases_init = ConstantInit(0.)\n",
    "\n",
    "    # Create a model with three hidden layers\n",
    "    model = MultipleLayerModel([\n",
    "        AffineLayer(input_dim, hidden_dim, weights_init, biases_init), \n",
    "        ReluLayer(),\n",
    "        AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "        ReluLayer(),\n",
    "        AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "        ReluLayer(),\n",
    "        AffineLayer(hidden_dim, output_dim, weights_init, biases_init)\n",
    "    ])\n",
    "\n",
    "    # Initialise a cross entropy error object\n",
    "    error = CrossEntropySoftmaxError()\n",
    "\n",
    "    # Use a basic gradient descent learning rule\n",
    "    learning_rule = AdamLearningRule(learning_rate=element[0], beta_1=element[1], beta_2=element[2])\n",
    "\n",
    "    # Remember to use notebook=False when you write a script to be run in a terminal\n",
    "    stats, keys, run_time, fig_1, ax_1, fig_2, ax_2 = train_model_and_plot_stats(\n",
    "        model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval, fig_name='Adam_' + str(i) + '.pdf', notebook=True)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    print('    final error(train) = {0:.2e}'.format(stats[-1, keys['error(train)']]))\n",
    "    print('    final error(valid) = {0:.2e}'.format(stats[-1, keys['error(valid)']]))\n",
    "    print('    final acc(train)   = {0:.2e}'.format(stats[-1, keys['acc(train)']]))\n",
    "    print('    final acc(valid)   = {0:.2e}'.format(stats[-1, keys['acc(valid)']]))\n",
    "    print('    run time per epoch = {0:.2f}'.format(run_time * 1. / num_epochs))\n",
    "\n",
    "    final_errors_train.append(stats[-1, keys['error(train)']])\n",
    "    final_errors_valid.append(stats[-1, keys['error(valid)']])\n",
    "    final_accs_train.append(stats[-1, keys['acc(train)']])\n",
    "    final_accs_valid.append(stats[-1, keys['acc(valid)']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 0\n",
    "print('| batch_size | learning_rate | dr_1 | dr_2 | final error(train) | final error(valid) | final acc(train) | final acc(valid) |')\n",
    "print('|------------|---------------|------|------|--------------------|--------------------|------------------|------------------|')\n",
    "for element in product(learning_rates, decay_rates_1, decay_rates_2):\n",
    "    print('| {0}   | {1:2f}   | {2:.2e}   | {3:.2e}   | {4:.2e}   | {5:.2e}   | {6:.2e}    | {7:.2f}      | {8:.2f}       |'\n",
    "          .format(j, batch_size, element[0], element[1], element[2], \n",
    "                  final_errors_train[j], final_errors_valid[j],\n",
    "                  final_accs_train[j], final_accs_valid[j]))\n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aditional training on Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup hyperparameters\n",
    "num_epochs = 100\n",
    "stats_interval = 1\n",
    "input_dim, output_dim, hidden_dim = 784, 47, 100\n",
    "\n",
    "learning_rates = [0.000001, 0.0000015, 0.00001, 0.000015]  # scale for random parameter initialisation\n",
    "decay_rates_1 = [0.99]#[0.85, 0.9, 0.95]\n",
    "decay_rates_2 = [0.999]#[0.95, 0.975, 0.999]\n",
    "final_errors_train = []\n",
    "final_errors_valid = []\n",
    "final_accs_train = []\n",
    "final_accs_valid = []\n",
    "\n",
    "for i, element in enumerate(product(learning_rates, decay_rates_1, decay_rates_2)):\n",
    "    \n",
    "    print('-' * 80)\n",
    "    print('learning_rate={0:.2e}, decay_rate={1:.2e}'\n",
    "          .format(element[0], element[1]))\n",
    "    print('-' * 80)\n",
    "    \n",
    "    # Reset random number generator and data provider states on each run\n",
    "    # to ensure reproducibility of results\n",
    "    rng.seed(seed)\n",
    "    train_data.reset()\n",
    "    valid_data.reset()\n",
    "\n",
    "    weights_init = GlorotUniformInit(rng=rng)\n",
    "    biases_init = ConstantInit(0.)\n",
    "\n",
    "    # Create a model with three hidden layers\n",
    "    model = MultipleLayerModel([\n",
    "        AffineLayer(input_dim, hidden_dim, weights_init, biases_init), \n",
    "        ReluLayer(),\n",
    "        AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "        ReluLayer(),\n",
    "        AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "        ReluLayer(),\n",
    "        AffineLayer(hidden_dim, output_dim, weights_init, biases_init)\n",
    "    ])\n",
    "\n",
    "    # Initialise a cross entropy error object\n",
    "    error = CrossEntropySoftmaxError()\n",
    "\n",
    "    # Use a basic gradient descent learning rule\n",
    "    learning_rule = AdamLearningRule(learning_rate=element[0], beta_1=element[1], beta_2=element[2])\n",
    "\n",
    "    # Remember to use notebook=False when you write a script to be run in a terminal\n",
    "    stats, keys, run_time, fig_1, ax_1, fig_2, ax_2 = train_model_and_plot_stats(\n",
    "        model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval, fig_name='Adam_' + str(i) + '.pdf', notebook=True)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    print('    final error(train) = {0:.2e}'.format(stats[-1, keys['error(train)']]))\n",
    "    print('    final error(valid) = {0:.2e}'.format(stats[-1, keys['error(valid)']]))\n",
    "    print('    final acc(train)   = {0:.2e}'.format(stats[-1, keys['acc(train)']]))\n",
    "    print('    final acc(valid)   = {0:.2e}'.format(stats[-1, keys['acc(valid)']]))\n",
    "    print('    run time per epoch = {0:.2f}'.format(run_time * 1. / num_epochs))\n",
    "\n",
    "    final_errors_train.append(stats[-1, keys['error(train)']])\n",
    "    final_errors_valid.append(stats[-1, keys['error(valid)']])\n",
    "    final_accs_train.append(stats[-1, keys['acc(train)']])\n",
    "    final_accs_valid.append(stats[-1, keys['acc(valid)']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 0\n",
    "print('| batch_size | learning_rate | dr_1 | dr_2 | final error(train) | final error(valid) | final acc(train) | final acc(valid) |')\n",
    "print('|------------|---------------|------|------|--------------------|--------------------|------------------|------------------|')\n",
    "for element in product(learning_rates, decay_rates_1, decay_rates_2):\n",
    "    print('| {0}   | {1:2f}   | {2:.2e}   | {3:.2e}   | {4:.2e}   | {5:.2e}   | {6:.2e}    | {7:.2f}      | {8:.2f}       |'\n",
    "          .format(j, batch_size, element[0], element[1], element[2], \n",
    "                  final_errors_train[j], final_errors_valid[j],\n",
    "                  final_accs_train[j], final_accs_valid[j]))\n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam With Weight Decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup hyperparameters\n",
    "learning_rate = 0.000015\n",
    "num_epochs = 100\n",
    "stats_interval = 1\n",
    "input_dim, output_dim, hidden_dim = 784, 47, 100\n",
    "\n",
    "\n",
    "print('-' * 80)\n",
    "print('learning_rate={0:.2e}'\n",
    "      .format(learning_rate))\n",
    "print('-' * 80)\n",
    "\n",
    "# Reset random number generator and data provider states on each run\n",
    "# to ensure reproducibility of results\n",
    "rng.seed(seed)\n",
    "train_data.reset()\n",
    "valid_data.reset()\n",
    "\n",
    "weights_init = GlorotUniformInit(rng=rng)\n",
    "biases_init = ConstantInit(0.)\n",
    "\n",
    "# Create a model with three hidden layers\n",
    "model = MultipleLayerModel([\n",
    "    AffineLayer(input_dim, hidden_dim, weights_init, biases_init), \n",
    "    ReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "    ReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "    ReluLayer(),\n",
    "    AffineLayer(hidden_dim, output_dim, weights_init, biases_init)\n",
    "])\n",
    "\n",
    "error = CrossEntropySoftmaxError()\n",
    "# Use a basic gradient descent learning rule\n",
    "learning_rule = AdamLearningRuleWithWeightDecay(learning_rate=learning_rate)\n",
    "\n",
    "#Remember to use notebook=False when you write a script to be run in a terminal\n",
    "stats, keys, run_time, fig_1, ax_1, fig_2, ax_2 = train_model_and_plot_stats(\n",
    "    model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval, fig_name='AdamWD_three_hidden_layers.pdf', notebook=True)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print('    final error(train) = {0:.2e}'.format(stats[-1, keys['error(train)']]))\n",
    "print('    final error(valid) = {0:.2e}'.format(stats[-1, keys['error(valid)']]))\n",
    "print('    final acc(train)   = {0:.2e}'.format(stats[-1, keys['acc(train)']]))\n",
    "print('    final acc(valid)   = {0:.2e}'.format(stats[-1, keys['acc(valid)']]))\n",
    "print('    run time per epoch = {0:.2f}'.format(run_time * 1. / num_epochs))\n",
    "\n",
    "# final_errors_train.append(stats[-1, keys['error(train)']])\n",
    "# final_errors_valid.append(stats[-1, keys['error(valid)']])\n",
    "# final_accs_train.append(stats[-1, keys['acc(train)']])\n",
    "# final_accs_valid.append(stats[-1, keys['acc(valid)']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST ALL THREE MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeysView(<numpy.lib.npyio.NpzFile object at 0x1039d88d0>)\n",
      "KeysView(<numpy.lib.npyio.NpzFile object at 0x114452eb8>)\n",
      "KeysView(<numpy.lib.npyio.NpzFile object at 0x114452eb8>)\n"
     ]
    }
   ],
   "source": [
    "# Set batch size\n",
    "batch_size = 100\n",
    "\n",
    "# Create data provider objects for the MNIST data set\n",
    "train_data = EMNISTDataProvider('train', batch_size=batch_size, rng=rng)\n",
    "valid_data = EMNISTDataProvider('valid', batch_size=batch_size, rng=rng)\n",
    "test_data = EMNISTDataProvider('test', batch_size=batch_size, rng=rng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "learning_rate=1.00e-02\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d44ba83bae8401482c9f241b5118b29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'update_learning_rule'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-389698be095d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;31m#Remember to use notebook=False when you write a script to be run in a terminal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m stats_1, keys_1, run_time_1 = test_model(\n\u001b[0;32m---> 39\u001b[0;31m     model, error, learning_rule, train_data, test_data, num_epochs, stats_interval, notebook=True)\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'    final error(train) = {0:.2e}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstats_1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys_1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error(train)'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-1f1f19a7b65c>\u001b[0m in \u001b[0;36mtest_model\u001b[0;34m(model, error, learning_rule, train_data, test_data, num_epochs, stats_interval, notebook)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# Run the optimiser for 5 epochs (full passes through the training set)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# printing statistics every epoch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mstats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimiser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstats_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstats_interval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/EDINBURGH/Semester 1/MLP/Assignments/Assignment1/MLP-Coursework1/mlp/optimisers.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, num_epochs, stats_interval)\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheduler\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_learning_rule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_number\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m                 \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_training_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'update_learning_rule'"
     ]
    }
   ],
   "source": [
    "#setup hyperparameters\n",
    "learning_rate = 0.01\n",
    "num_epochs = 100\n",
    "stats_interval = 1\n",
    "input_dim, output_dim, hidden_dim = 784, 47, 100\n",
    "\n",
    "\n",
    "print('-' * 80)\n",
    "print('learning_rate={0:.2e}'\n",
    "      .format(learning_rate))\n",
    "print('-' * 80)\n",
    "\n",
    "# Reset random number generator and data provider states on each run\n",
    "# to ensure reproducibility of results\n",
    "rng.seed(seed)\n",
    "train_data.reset()\n",
    "test_data.reset()\n",
    "\n",
    "weights_init = GlorotUniformInit(rng=rng)\n",
    "biases_init = ConstantInit(0.)\n",
    "\n",
    "# Create a model with three hidden layers\n",
    "model = MultipleLayerModel([\n",
    "    AffineLayer(input_dim, hidden_dim, weights_init, biases_init), \n",
    "    ReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "    ReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "    ReluLayer(),\n",
    "    AffineLayer(hidden_dim, output_dim, weights_init, biases_init)\n",
    "])\n",
    "\n",
    "error = CrossEntropySoftmaxError()\n",
    "# Use a basic gradient descent learning rule\n",
    "learning_rule = GradientDescentLearningRule(learning_rate=learning_rate)\n",
    "\n",
    "#Remember to use notebook=False when you write a script to be run in a terminal\n",
    "stats_1, keys_1, run_time_1 = test_model(\n",
    "    model, error, learning_rule, train_data, test_data, num_epochs, stats_interval, notebook=True)\n",
    "\n",
    "print('    final error(train) = {0:.2e}'.format(stats_1[-1, keys_1['error(train)']]))\n",
    "print('    final error(test)  = {0:.2e}'.format(stats_1[-1, keys_1['error(valid)']]))\n",
    "print('    final acc(train)   = {0:.2e}'.format(stats_1[-1, keys_1['acc(train)']]))\n",
    "print('    final acc(test)    = {0:.2e}'.format(stats_1[-1, keys_1['acc(valid)']]))\n",
    "print('    run time per epoch = {0:.2f}'.format(run_time_1 * 1. / num_epochs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup hyperparameters\n",
    "learning_rate = 0.00001\n",
    "decay_rate = 0.9\n",
    "num_epochs = 100\n",
    "stats_interval = 1\n",
    "input_dim, output_dim, hidden_dim = 784, 47, 100\n",
    "\n",
    "\n",
    "print('-' * 80)\n",
    "print('learning_rate={0:.2e}, decay_rate={1:.2e}'\n",
    "      .format(learning_rate, decay_rate))\n",
    "print('-' * 80)\n",
    "\n",
    "# Reset random number generator and data provider states on each run\n",
    "# to ensure reproducibility of results\n",
    "rng.seed(seed)\n",
    "train_data.reset()\n",
    "valid_data.reset()\n",
    "\n",
    "weights_init = GlorotUniformInit(rng=rng)\n",
    "biases_init = ConstantInit(0.)\n",
    "\n",
    "# Create a model with three hidden layers\n",
    "model = MultipleLayerModel([\n",
    "    AffineLayer(input_dim, hidden_dim, weights_init, biases_init), \n",
    "    ReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "    ReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "    ReluLayer(),\n",
    "    AffineLayer(hidden_dim, output_dim, weights_init, biases_init)\n",
    "])\n",
    "\n",
    "error = CrossEntropySoftmaxError()\n",
    "# Use a basic gradient descent learning rule\n",
    "# learning_rule = RMSPropLearningRule()\n",
    "learning_rule = RMSPropLearningRule(learning_rate=learning_rate, beta=decay_rate)\n",
    "\n",
    "#Remember to use notebook=False when you write a script to be run in a terminal\n",
    "stats_2, keys_2, run_time_2 = test_model(\n",
    "    model, error, learning_rule, train_data, test_data, num_epochs, stats_interval, notebook=True)\n",
    "\n",
    "print('    final error(train) = {0:.2e}'.format(stats_2[-1, keys_2['error(train)']]))\n",
    "print('    final error(test)  = {0:.2e}'.format(stats_2[-1, keys_2['error(valid)']]))\n",
    "print('    final acc(train)   = {0:.2e}'.format(stats_2[-1, keys_2['acc(train)']]))\n",
    "print('    final acc(test)    = {0:.2e}'.format(stats_2[-1, keys_2['acc(valid)']]))\n",
    "print('    run time per epoch = {0:.2f}'.format(run_time_2 * 1. / num_epochs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "learning_rate=1.50e-05, decay_rate_1=9.00e-01, decay_rate_2=9.50e-01\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3f8c0a2fa924961ad5793cb5d6015f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'update_learning_rule'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-1654fdefe9e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;31m#Remember to use notebook=False when you write a script to be run in a terminal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m stats_3, keys_3, run_time_3 = test_model(\n\u001b[0;32m---> 42\u001b[0;31m     model, error, learning_rule, train_data, test_data, num_epochs, stats_interval, notebook=True)\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'    final error(train) = {0:.2e}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstats_3\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys_3\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error(train)'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-1f1f19a7b65c>\u001b[0m in \u001b[0;36mtest_model\u001b[0;34m(model, error, learning_rule, train_data, test_data, num_epochs, stats_interval, notebook)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# Run the optimiser for 5 epochs (full passes through the training set)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# printing statistics every epoch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mstats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimiser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstats_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstats_interval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/EDINBURGH/Semester 1/MLP/Assignments/Assignment1/MLP-Coursework1/mlp/optimisers.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, num_epochs, stats_interval)\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheduler\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_learning_rule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_number\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m                 \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_training_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'update_learning_rule'"
     ]
    }
   ],
   "source": [
    "#setup hyperparameters\n",
    "learning_rate = 0.000015\n",
    "decay_rate_1 = 0.9\n",
    "decay_rate_2 = 0.95\n",
    "num_epochs = 100\n",
    "stats_interval = 1\n",
    "input_dim, output_dim, hidden_dim = 784, 47, 100\n",
    "\n",
    "\n",
    "print('-' * 80)\n",
    "print('learning_rate={0:.2e}, decay_rate_1={1:.2e}, decay_rate_2={2:.2e}'\n",
    "      .format(learning_rate, decay_rate_1, decay_rate_2))\n",
    "print('-' * 80)\n",
    "\n",
    "# Reset random number generator and data provider states on each run\n",
    "# to ensure reproducibility of results\n",
    "rng.seed(seed)\n",
    "train_data.reset()\n",
    "valid_data.reset()\n",
    "\n",
    "weights_init = GlorotUniformInit(rng=rng)\n",
    "biases_init = ConstantInit(0.)\n",
    "\n",
    "# Create a model with three hidden layers\n",
    "model = MultipleLayerModel([\n",
    "    AffineLayer(input_dim, hidden_dim, weights_init, biases_init), \n",
    "    ReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "    ReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "    ReluLayer(),\n",
    "    AffineLayer(hidden_dim, output_dim, weights_init, biases_init)\n",
    "])\n",
    "\n",
    "error = CrossEntropySoftmaxError()\n",
    "# Use a basic gradient descent learning rule\n",
    "# learning_rule = AdamLearningRule()\n",
    "learning_rule = AdamLearningRule(learning_rate=learning_rate, beta_1=decay_rate_1, beta_2=decay_rate_2)\n",
    "\n",
    "#Remember to use notebook=False when you write a script to be run in a terminal\n",
    "stats_3, keys_3, run_time_3 = test_model(\n",
    "    model, error, learning_rule, train_data, test_data, num_epochs, stats_interval, notebook=True)\n",
    "\n",
    "print('    final error(train) = {0:.2e}'.format(stats_3[-1, keys_3['error(train)']]))\n",
    "print('    final error(test)  = {0:.2e}'.format(stats_3[-1, keys_3['error(valid)']]))\n",
    "print('    final acc(train)   = {0:.2e}'.format(stats_3[-1, keys_3['acc(train)']]))\n",
    "print('    final acc(test)    = {0:.2e}'.format(stats_3[-1, keys_3['acc(valid)']]))\n",
    "print('    run time per epoch = {0:.2f}'.format(run_time_3 * 1. / num_epochs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the change in the validation and training set error over training.\n",
    "fig_1 = plt.figure(figsize=(8, 4))\n",
    "ax_1 = fig_1.add_subplot(111)\n",
    "for k in ['error(valid)']:\n",
    "    ax_1.plot(np.arange(1, stats_1.shape[0]) * stats_interval, \n",
    "              stats_1[1:, keys_1[k]], label='SGD: error(test)')\n",
    "    ax_1.plot(np.arange(1, stats_2.shape[0]) * stats_interval, \n",
    "              stats_2[1:, keys_2[k]], label='RMSprop: error(test)')\n",
    "    ax_1.plot(np.arange(1, stats_3.shape[0]) * stats_interval, \n",
    "              stats_3[1:, keys_3[k]], label='Adam: error(test)')\n",
    "#     ax_1.legend(loc=0)\n",
    "ax_1.grid('on') # Turn axes grid on\n",
    "ax_1.legend(loc='best', fontsize=11) # Add a legend\n",
    "ax_1.set_xlabel('Epoch number')\n",
    "\n",
    "fig_1.tight_layout() # This minimises whitespace around the axes.\n",
    "fig_1.savefig('err_test_models') # Save figure to current directory in PDF format\n",
    "\n",
    "# Plot the change in the validation and training set accuracy over training.\n",
    "fig_2 = plt.figure(figsize=(8, 4))\n",
    "ax_2 = fig_2.add_subplot(111)\n",
    "for k in ['acc(valid)']:\n",
    "    ax_2.plot(np.arange(1, stats_1.shape[0]) * stats_interval, \n",
    "              stats_1[1:, keys_1[k]], label='SGD: acc(test)')\n",
    "    ax_2.plot(np.arange(1, stats_2.shape[0]) * stats_interval, \n",
    "              stats_2[1:, keys_2[k]], label='RMSprop: acc(test)')\n",
    "    ax_2.plot(np.arange(1, stats_3.shape[0]) * stats_interval, \n",
    "              stats_3[1:, keys_3[k]], label='Adam: acc(test)')\n",
    "#     ax_2.legend(loc=0)\n",
    "ax_2.grid('on') # Turn axes grid on\n",
    "ax_2.legend(loc='best', fontsize=11) # Add a legend\n",
    "ax_2.set_xlabel('Epoch number')\n",
    "\n",
    "fig_2.tight_layout() # This minimises whitespace around the axes.\n",
    "fig_2.savefig('acc_test_models') # Save figure to current directory in PDF format\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adagrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup hyperparameters\n",
    "learning_rate = 0.000015\n",
    "decay_rate_1 = 0.9\n",
    "decay_rate_2 = 0.95\n",
    "num_epochs = 100\n",
    "stats_interval = 1\n",
    "input_dim, output_dim, hidden_dim = 784, 47, 100\n",
    "\n",
    "\n",
    "print('-' * 80)\n",
    "print('learning_rate={0:.2e}, decay_rate_1={1:.2e}, decay_rate_2={2:.2e}'\n",
    "      .format(learning_rate, decay_rate_1, decay_rate_2))\n",
    "print('-' * 80)\n",
    "\n",
    "# Reset random number generator and data provider states on each run\n",
    "# to ensure reproducibility of results\n",
    "rng.seed(seed)\n",
    "train_data.reset()\n",
    "valid_data.reset()\n",
    "\n",
    "weights_init = GlorotUniformInit(rng=rng)\n",
    "biases_init = ConstantInit(0.)\n",
    "\n",
    "# Create a model with three hidden layers\n",
    "model = MultipleLayerModel([\n",
    "    AffineLayer(input_dim, hidden_dim, weights_init, biases_init), \n",
    "    ReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "    ReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "    ReluLayer(),\n",
    "    AffineLayer(hidden_dim, output_dim, weights_init, biases_init)\n",
    "])\n",
    "\n",
    "error = CrossEntropySoftmaxError()\n",
    "# Use a basic gradient descent learning rule\n",
    "# learning_rule = AdamLearningRule()\n",
    "learning_rule = AdaGradLearningRule()\n",
    "\n",
    "#Remember to use notebook=False when you write a script to be run in a terminal\n",
    "stats_4, keys_4, run_time_4 = test_model(\n",
    "    model, error, learning_rule, train_data, test_data, num_epochs, stats_interval, notebook=True)\n",
    "\n",
    "print('    final error(train) = {0:.2e}'.format(stats_4[-1, keys_4['error(train)']]))\n",
    "print('    final error(test)  = {0:.2e}'.format(stats_4[-1, keys_4['error(valid)']]))\n",
    "print('    final acc(train)   = {0:.2e}'.format(stats_4[-1, keys_4['acc(train)']]))\n",
    "print('    final acc(test)    = {0:.2e}'.format(stats_4[-1, keys_4['acc(valid)']]))\n",
    "print('    run time per epoch = {0:.2f}'.format(run_time_4 * 1. / num_epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a0afd856a4aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Plot the change in the validation and training set error over training.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfig_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0max_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m111\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'error(valid)'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     ax_1.plot(np.arange(1, stats_1.shape[0]) * stats_interval, \n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# Plot the change in the validation and training set error over training.\n",
    "fig_1 = plt.figure(figsize=(8, 4))\n",
    "ax_1 = fig_1.add_subplot(111)\n",
    "for k in ['error(valid)']:\n",
    "    ax_1.plot(np.arange(1, stats_1.shape[0]) * stats_interval, \n",
    "              stats_1[1:, keys_1[k]], label='SGD: error(test)')\n",
    "    ax_1.plot(np.arange(1, stats_2.shape[0]) * stats_interval, \n",
    "              stats_2[1:, keys_2[k]], label='RMSprop: error(test)')\n",
    "    ax_1.plot(np.arange(1, stats_3.shape[0]) * stats_interval, \n",
    "              stats_3[1:, keys_3[k]], label='Adam: error(test)')\n",
    "    ax_1.plot(np.arange(1, stats_4.shape[0]) * stats_interval, \n",
    "              stats_4[1:, keys_4[k]], label='Adagrad: error(test)')\n",
    "#     ax_1.legend(loc=0)\n",
    "ax_1.grid('on') # Turn axes grid on\n",
    "ax_1.legend(loc='best', fontsize=11) # Add a legend\n",
    "ax_1.set_xlabel('Epoch number')\n",
    "\n",
    "fig_1.tight_layout() # This minimises whitespace around the axes.\n",
    "fig_1.savefig('full_err_test_models.pdf') # Save figure to current directory in PDF format\n",
    "\n",
    "# Plot the change in the validation and training set accuracy over training.\n",
    "fig_2 = plt.figure(figsize=(8, 4))\n",
    "ax_2 = fig_2.add_subplot(111)\n",
    "for k in ['acc(valid)']:\n",
    "    ax_2.plot(np.arange(1, stats_1.shape[0]) * stats_interval, \n",
    "              stats_1[1:, keys_1[k]], label='SGD: acc(test)')\n",
    "    ax_2.plot(np.arange(1, stats_2.shape[0]) * stats_interval, \n",
    "              stats_2[1:, keys_2[k]], label='RMSprop: acc(test)')\n",
    "    ax_2.plot(np.arange(1, stats_3.shape[0]) * stats_interval, \n",
    "              stats_3[1:, keys_3[k]], label='Adam: acc(test)')\n",
    "    ax_2.plot(np.arange(1, stats_4.shape[0]) * stats_interval, \n",
    "              stats_4[1:, keys_4[k]], label='Adagrad: acc(test)')\n",
    "#     ax_2.legend(loc=0)\n",
    "ax_2.grid('on') # Turn axes grid on\n",
    "ax_2.legend(loc='best', fontsize=11) # Add a legend\n",
    "ax_2.set_xlabel('Epoch number')\n",
    "\n",
    "fig_2.tight_layout() # This minimises whitespace around the axes.\n",
    "fig_2.savefig('full_acc_test_models.pdf') # Save figure to current directory in PDF format\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [0.000001, 0.0000015, 0.00001, 0.000015]  # scale for random parameter initialisation\n",
    "decay_rates_1 = [0.99, 0,999]#[0.85, 0.9, 0.95]\n",
    "decay_rates_2 = [0.999]#[0.95, 0.975, 0.999]\n",
    "\n",
    "for i, element in enumerate(product(learning_rates, decay_rates_1, decay_rates_2)):\n",
    "    print(i, element[0], element[1], element[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from itertools import product\n",
    "a = np.linspace(0.1, 0.9, 5) #np.array([1, 2, 3, 4, 5 , 6])\n",
    "b = [0.000001, 0.0000015, 0.00001, 0.000015, 0.0001, 0.00015] #np.array(['a', 'b', 'c'])\n",
    "\n",
    "for i, element in enumerate(product(a, b)):\n",
    "    print(i, element)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
