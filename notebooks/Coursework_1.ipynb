{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coursework 1\n",
    "\n",
    "This notebook is intended to be used as a starting point for your experiments. The instructions can be found in the instructions file located under spec/coursework1.pdf. The methods provided here are just helper functions. If you want more complex graphs such as side by side comparisons of different experiments you should learn more about matplotlib and implement them. Before each experiment remember to re-initialize neural network weights and reset the data providers so you get a properly initialized experiment. For each experiment try to keep most hyperparameters the same except the one under investigation so you can understand what the effects of each are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "def train_model_and_plot_stats(\n",
    "        model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval, fig_name, notebook=True):\n",
    "    \n",
    "    # As well as monitoring the error over training also monitor classification\n",
    "    # accuracy i.e. proportion of most-probable predicted classes being equal to targets\n",
    "    data_monitors={'acc': lambda y, t: (y.argmax(-1) == t.argmax(-1)).mean()}\n",
    "\n",
    "    # Use the created objects to initialise a new Optimiser instance.\n",
    "    optimiser = Optimiser(\n",
    "        model, error, learning_rule, train_data, valid_data, data_monitors, notebook=notebook)\n",
    "\n",
    "    # Run the optimiser for 5 epochs (full passes through the training set)\n",
    "    # printing statistics every epoch.\n",
    "    stats, keys, run_time = optimiser.train(num_epochs=num_epochs, stats_interval=stats_interval)\n",
    "\n",
    "    # Plot the change in the validation and training set error over training.\n",
    "    fig_1 = plt.figure(figsize=(8, 4))\n",
    "    ax_1 = fig_1.add_subplot(111)\n",
    "    for k in ['error(train)', 'error(valid)']:\n",
    "        ax_1.plot(np.arange(1, stats.shape[0]) * stats_interval, \n",
    "                  stats[1:, keys[k]], label=k)\n",
    "#     ax_1.legend(loc=0)\n",
    "    ax_1.grid('on') # Turn axes grid on\n",
    "    ax_1.legend(loc='best', fontsize=11) # Add a legend\n",
    "    ax_1.set_xlabel('Epoch number')\n",
    "\n",
    "    fig_1.tight_layout() # This minimises whitespace around the axes.\n",
    "    fig_1.savefig('err_' + fig_name) # Save figure to current directory in PDF format\n",
    "    \n",
    "    # Plot the change in the validation and training set accuracy over training.\n",
    "    fig_2 = plt.figure(figsize=(8, 4))\n",
    "    ax_2 = fig_2.add_subplot(111)\n",
    "    for k in ['acc(train)', 'acc(valid)']:\n",
    "        ax_2.plot(np.arange(1, stats.shape[0]) * stats_interval, \n",
    "                  stats[1:, keys[k]], label=k)\n",
    "#     ax_2.legend(loc=0)\n",
    "    ax_2.grid('on') # Turn axes grid on\n",
    "    ax_2.legend(loc='best', fontsize=11) # Add a legend\n",
    "    ax_2.set_xlabel('Epoch number')\n",
    "    \n",
    "    fig_2.tight_layout() # This minimises whitespace around the axes.\n",
    "    fig_2.savefig('acc_' + fig_name) # Save figure to current directory in PDF format\n",
    "    \n",
    "    return stats, keys, run_time, fig_1, ax_1, fig_2, ax_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "def train_test_model_and_plot_stats(\n",
    "        model, error, learning_rule, train_data, valid_data, test_data, num_epochs, stats_interval, fig_name, notebook=True):\n",
    "    \n",
    "    # As well as monitoring the error over training also monitor classification\n",
    "    # accuracy i.e. proportion of most-probable predicted classes being equal to targets\n",
    "    data_monitors={'acc': lambda y, t: (y.argmax(-1) == t.argmax(-1)).mean()}\n",
    "\n",
    "    # Use the created objects to initialise a new Optimiser instance.\n",
    "    optimiser = Optimiser(\n",
    "        model, error, learning_rule, train_data, valid_data, data_monitors, notebook=notebook)\n",
    "\n",
    "    # Run the optimiser for 5 epochs (full passes through the training set)\n",
    "    # printing statistics every epoch.\n",
    "    stats, keys, run_time = optimiser.train(num_epochs=num_epochs, stats_interval=stats_interval)\n",
    "    \n",
    "    \n",
    "    dict_res = optimiser.eval_monitors(dataset=test_data, label='(test)')\n",
    "\n",
    "    # Plot the change in the validation and training set error over training.\n",
    "    fig_1 = plt.figure(figsize=(8, 4))\n",
    "    ax_1 = fig_1.add_subplot(111)\n",
    "    for k in ['error(train)', 'error(valid)']:\n",
    "        ax_1.plot(np.arange(1, stats.shape[0]) * stats_interval, \n",
    "                  stats[1:, keys[k]], label=k)\n",
    "#     ax_1.legend(loc=0)\n",
    "    ax_1.grid('on') # Turn axes grid on\n",
    "    ax_1.legend(loc='best', fontsize=11) # Add a legend\n",
    "    ax_1.set_xlabel('Epoch number')\n",
    "\n",
    "    fig_1.tight_layout() # This minimises whitespace around the axes.\n",
    "    fig_1.savefig('err_' + fig_name) # Save figure to current directory in PDF format\n",
    "    \n",
    "    # Plot the change in the validation and training set accuracy over training.\n",
    "    fig_2 = plt.figure(figsize=(8, 4))\n",
    "    ax_2 = fig_2.add_subplot(111)\n",
    "    for k in ['acc(train)', 'acc(valid)']:\n",
    "        ax_2.plot(np.arange(1, stats.shape[0]) * stats_interval, \n",
    "                  stats[1:, keys[k]], label=k)\n",
    "#     ax_2.legend(loc=0)\n",
    "    ax_2.grid('on') # Turn axes grid on\n",
    "    ax_2.legend(loc='best', fontsize=11) # Add a legend\n",
    "    ax_2.set_xlabel('Epoch number')\n",
    "    \n",
    "    fig_2.tight_layout() # This minimises whitespace around the axes.\n",
    "    fig_2.savefig('acc_' + fig_name) # Save figure to current directory in PDF format\n",
    "    \n",
    "    return stats, keys, run_time, fig_1, ax_1, fig_2, ax_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The below code will set up the data providers, random number\n",
    "# generator and logger objects needed for training runs. As\n",
    "# loading the data from file take a little while you generally\n",
    "# will probably not want to reload the data providers on\n",
    "# every training run. If you wish to reset their state you\n",
    "# should instead use the .reset() method of the data providers.\n",
    "import numpy as np\n",
    "import logging\n",
    "from mlp.data_providers import MNISTDataProvider, EMNISTDataProvider\n",
    "\n",
    "# Seed a random number generator\n",
    "seed = 11102018 \n",
    "rng = np.random.RandomState(seed)\n",
    "# Set up a logger object to print info about the training run to stdout\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.handlers = [logging.StreamHandler()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DON'T RUN THIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model set up code below is provided as a starting point.\n",
    "# You will probably want to add further code cells for the\n",
    "# different experiments you run.\n",
    "\n",
    "from mlp.layers import AffineLayer, SoftmaxLayer, SigmoidLayer, ReluLayer, LeakyReluLayer\n",
    "from mlp.errors import CrossEntropySoftmaxError\n",
    "from mlp.models import MultipleLayerModel\n",
    "from mlp.initialisers import ConstantInit, GlorotUniformInit\n",
    "from mlp.learning_rules import AdamLearningRule\n",
    "from mlp.optimisers import Optimiser\n",
    "\n",
    "#setup hyperparameters\n",
    "learning_rate = 0.1\n",
    "num_epochs = 100\n",
    "stats_interval = 1\n",
    "input_dim, output_dim, hidden_dim = 784, 47, 100\n",
    "\n",
    "# Reset random number generator and data provider states on each run\n",
    "# to ensure reproducibility of results\n",
    "rng.seed(seed)\n",
    "train_data.reset()\n",
    "valid_data.reset()\n",
    "\n",
    "weights_init = GlorotUniformInit(rng=rng)\n",
    "biases_init = ConstantInit(0.)\n",
    "model = MultipleLayerModel([\n",
    "    AffineLayer(input_dim, hidden_dim, weights_init, biases_init), \n",
    "    ReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "    ReluLayer(),\n",
    "    AffineLayer(hidden_dim, output_dim, weights_init, biases_init)\n",
    "])\n",
    "\n",
    "error = CrossEntropySoftmaxError()\n",
    "# Use a basic gradient descent learning rule\n",
    "learning_rule = AdamLearningRule()\n",
    "\n",
    "#Remember to use notebook=False when you write a script to be run in a terminal\n",
    "_ = train_model_and_plot_stats(\n",
    "    model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval, notebook=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RUN THIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model set up code below is provided as a starting point.\n",
    "# You will probably want to add further code cells for the\n",
    "# different experiments you run.\n",
    "\n",
    "from mlp.layers import AffineLayer, SoftmaxLayer, SigmoidLayer, ReluLayer, LeakyReluLayer\n",
    "from mlp.errors import CrossEntropySoftmaxError\n",
    "from mlp.models import MultipleLayerModel\n",
    "from mlp.initialisers import ConstantInit, GlorotUniformInit\n",
    "from mlp.learning_rules import AdamLearningRule, GradientDescentLearningRule\n",
    "from mlp.optimisers import Optimiser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set batch size\n",
    "batch_size = 100\n",
    "\n",
    "# Create data provider objects for the MNIST data set\n",
    "train_data = EMNISTDataProvider('train', batch_size=batch_size, rng=rng)\n",
    "valid_data = EMNISTDataProvider('valid', batch_size=batch_size, rng=rng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup hyperparameters\n",
    "num_epochs = 100\n",
    "stats_interval = 1\n",
    "input_dim, output_dim, hidden_dim = 784, 47, 100\n",
    "\n",
    "learning_rates = [0.0001, 0.001, 0.01, 0.015, 0.1, 0.2, 0.5]  # scale for random parameter initialisation\n",
    "final_errors_train = []\n",
    "final_errors_valid = []\n",
    "final_accs_train = []\n",
    "final_accs_valid = []\n",
    "\n",
    "for i, learning_rate in enumerate(learning_rates):\n",
    "\n",
    "    print('-' * 80)\n",
    "    print('learning_rate={0:.2e}'\n",
    "          .format(learning_rate))\n",
    "    print('-' * 80)\n",
    "    \n",
    "    # Reset random number generator and data provider states on each run\n",
    "    # to ensure reproducibility of results\n",
    "    rng.seed(seed)\n",
    "    train_data.reset()\n",
    "    valid_data.reset()\n",
    "\n",
    "    weights_init = GlorotUniformInit(rng=rng)\n",
    "    biases_init = ConstantInit(0.)\n",
    "\n",
    "    # Create a model with two hidden layers\n",
    "    model = MultipleLayerModel([\n",
    "        AffineLayer(input_dim, hidden_dim, weights_init, biases_init), \n",
    "        ReluLayer(),\n",
    "        AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "        ReluLayer(),\n",
    "        AffineLayer(hidden_dim, output_dim, weights_init, biases_init)\n",
    "    ])\n",
    "\n",
    "    # Initialise a cross entropy error object\n",
    "    error = CrossEntropySoftmaxError()\n",
    "\n",
    "    # Use a basic gradient descent learning rule\n",
    "    learning_rule = GradientDescentLearningRule(learning_rate=learning_rate)\n",
    "\n",
    "    # Remember to use notebook=False when you write a script to be run in a terminal\n",
    "    stats, keys, run_time, fig_1, ax_1, fig_2, ax_2 = train_model_and_plot_stats(\n",
    "        model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval, fig_name='two_hidden_layers_' + str(i) + '.pdf', notebook=True)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    print('    final error(train) = {0:.2e}'.format(stats[-1, keys['error(train)']]))\n",
    "    print('    final error(valid) = {0:.2e}'.format(stats[-1, keys['error(valid)']]))\n",
    "    print('    final acc(train)   = {0:.2e}'.format(stats[-1, keys['acc(train)']]))\n",
    "    print('    final acc(valid)   = {0:.2e}'.format(stats[-1, keys['acc(valid)']]))\n",
    "    print('    run time per epoch = {0:.2f}'.format(run_time * 1. / num_epochs))\n",
    "\n",
    "    final_errors_train.append(stats[-1, keys['error(train)']])\n",
    "    final_errors_valid.append(stats[-1, keys['error(valid)']])\n",
    "    final_accs_train.append(stats[-1, keys['acc(train)']])\n",
    "    final_accs_valid.append(stats[-1, keys['acc(valid)']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 0\n",
    "print('| batch_size | learning_rate | final error(train) | final error(valid) | final acc(train) | final acc(valid) |')\n",
    "print('|------------|---------------|--------------------|--------------------|------------------|------------------|')\n",
    "for learning_rate in learning_rates:\n",
    "    print('| {0:f}        | {1:.2e}        | {2:.2e}           | {3:.2e}           |  {4:.2f}            | {5:.2f}             |'\n",
    "          .format(batch_size, learning_rate, \n",
    "                  final_errors_train[j], final_errors_valid[j],\n",
    "                  final_accs_train[j], final_accs_valid[j]))\n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup hyperparameters\n",
    "num_epochs = 100\n",
    "stats_interval = 1\n",
    "input_dim, output_dim, hidden_dim = 784, 47, 100\n",
    "\n",
    "learning_rates = [0.0001, 0.001, 0.01, 0.015, 0.1, 0.2, 0.5]  # scale for random parameter initialisation\n",
    "final_errors_train = []\n",
    "final_errors_valid = []\n",
    "final_accs_train = []\n",
    "final_accs_valid = []\n",
    "\n",
    "for i, learning_rate in enumerate(learning_rates):\n",
    "\n",
    "    print('-' * 80)\n",
    "    print('learning_rate={0:.2e}'\n",
    "          .format(learning_rate))\n",
    "    print('-' * 80)\n",
    "    \n",
    "    # Reset random number generator and data provider states on each run\n",
    "    # to ensure reproducibility of results\n",
    "    rng.seed(seed)\n",
    "    train_data.reset()\n",
    "    valid_data.reset()\n",
    "\n",
    "    weights_init = GlorotUniformInit(rng=rng)\n",
    "    biases_init = ConstantInit(0.)\n",
    "\n",
    "    # Create a model with three hidden layers\n",
    "    model = MultipleLayerModel([\n",
    "        AffineLayer(input_dim, hidden_dim, weights_init, biases_init), \n",
    "        ReluLayer(),\n",
    "        AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "        ReluLayer(),\n",
    "        AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "        ReluLayer(),\n",
    "        AffineLayer(hidden_dim, output_dim, weights_init, biases_init)\n",
    "    ])\n",
    "\n",
    "    # Initialise a cross entropy error object\n",
    "    error = CrossEntropySoftmaxError()\n",
    "\n",
    "    # Use a basic gradient descent learning rule\n",
    "    learning_rule = GradientDescentLearningRule(learning_rate=learning_rate)\n",
    "\n",
    "    # Remember to use notebook=False when you write a script to be run in a terminal\n",
    "    stats, keys, run_time, fig_1, ax_1, fig_2, ax_2 = train_model_and_plot_stats(\n",
    "        model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval, fig_name='three_hidden_layers_' + str(i) + '.pdf', notebook=True)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    print('    final error(train) = {0:.2e}'.format(stats[-1, keys['error(train)']]))\n",
    "    print('    final error(valid) = {0:.2e}'.format(stats[-1, keys['error(valid)']]))\n",
    "    print('    final acc(train)   = {0:.2e}'.format(stats[-1, keys['acc(train)']]))\n",
    "    print('    final acc(valid)   = {0:.2e}'.format(stats[-1, keys['acc(valid)']]))\n",
    "    print('    run time per epoch = {0:.2f}'.format(run_time * 1. / num_epochs))\n",
    "\n",
    "    final_errors_train.append(stats[-1, keys['error(train)']])\n",
    "    final_errors_valid.append(stats[-1, keys['error(valid)']])\n",
    "    final_accs_train.append(stats[-1, keys['acc(train)']])\n",
    "    final_accs_valid.append(stats[-1, keys['acc(valid)']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 0\n",
    "print('| batch_size | learning_rate | final error(train) | final error(valid) | final acc(train) | final acc(valid) |')\n",
    "print('|------------|---------------|--------------------|--------------------|------------------|------------------|')\n",
    "for learning_rate in learning_rates:\n",
    "    print('| {0:f}        | {1:.2e}        | {2:.2e}           | {3:.2e}           |  {4:.2f}            | {5:.2f}             |'\n",
    "          .format(batch_size, learning_rate, \n",
    "                  final_errors_train[j], final_errors_valid[j],\n",
    "                  final_accs_train[j], final_accs_valid[j]))\n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup hyperparameters\n",
    "num_epochs = 100\n",
    "stats_interval = 1\n",
    "input_dim, output_dim, hidden_dim = 784, 47, 100\n",
    "\n",
    "learning_rates = [0.0001, 0.001, 0.01, 0.015, 0.1, 0.2, 0.5]  # scale for random parameter initialisation\n",
    "final_errors_train = []\n",
    "final_errors_valid = []\n",
    "final_accs_train = []\n",
    "final_accs_valid = []\n",
    "\n",
    "for i, learning_rate in enumerate(learning_rates):\n",
    "\n",
    "    print('-' * 80)\n",
    "    print('learning_rate={0:.2e}'\n",
    "          .format(learning_rate))\n",
    "    print('-' * 80)\n",
    "    \n",
    "    # Reset random number generator and data provider states on each run\n",
    "    # to ensure reproducibility of results\n",
    "    rng.seed(seed)\n",
    "    train_data.reset()\n",
    "    valid_data.reset()\n",
    "\n",
    "    weights_init = GlorotUniformInit(rng=rng)\n",
    "    biases_init = ConstantInit(0.)\n",
    "\n",
    "    # Create a model with four hidden layers\n",
    "    model = MultipleLayerModel([\n",
    "        AffineLayer(input_dim, hidden_dim, weights_init, biases_init), \n",
    "        ReluLayer(),\n",
    "        AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "        ReluLayer(),\n",
    "        AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "        ReluLayer(),\n",
    "        AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "        ReluLayer(),\n",
    "        AffineLayer(hidden_dim, output_dim, weights_init, biases_init)\n",
    "    ])\n",
    "\n",
    "    # Initialise a cross entropy error object\n",
    "    error = CrossEntropySoftmaxError()\n",
    "\n",
    "    # Use a basic gradient descent learning rule\n",
    "    learning_rule = GradientDescentLearningRule(learning_rate=learning_rate)\n",
    "\n",
    "    # Remember to use notebook=False when you write a script to be run in a terminal\n",
    "    stats, keys, run_time, fig_1, ax_1, fig_2, ax_2 = train_model_and_plot_stats(\n",
    "        model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval, fig_name='four_hidden_layers_' + str(i) + '.pdf', notebook=True)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    print('    final error(train) = {0:.2e}'.format(stats[-1, keys['error(train)']]))\n",
    "    print('    final error(valid) = {0:.2e}'.format(stats[-1, keys['error(valid)']]))\n",
    "    print('    final acc(train)   = {0:.2e}'.format(stats[-1, keys['acc(train)']]))\n",
    "    print('    final acc(valid)   = {0:.2e}'.format(stats[-1, keys['acc(valid)']]))\n",
    "    print('    run time per epoch = {0:.2f}'.format(run_time * 1. / num_epochs))\n",
    "\n",
    "    final_errors_train.append(stats[-1, keys['error(train)']])\n",
    "    final_errors_valid.append(stats[-1, keys['error(valid)']])\n",
    "    final_accs_train.append(stats[-1, keys['acc(train)']])\n",
    "    final_accs_valid.append(stats[-1, keys['acc(valid)']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 0\n",
    "print('| batch_size | learning_rate | final error(train) | final error(valid) | final acc(train) | final acc(valid) |')\n",
    "print('|------------|---------------|--------------------|--------------------|------------------|------------------|')\n",
    "for learning_rate in learning_rates:\n",
    "    print('| {0:f}        | {1:.2e}        | {2:.2e}           | {3:.2e}           |  {4:.2f}            | {5:.2f}             |'\n",
    "          .format(batch_size, learning_rate, \n",
    "                  final_errors_train[j], final_errors_valid[j],\n",
    "                  final_accs_train[j], final_accs_valid[j]))\n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup hyperparameters\n",
    "num_epochs = 100\n",
    "stats_interval = 1\n",
    "input_dim, output_dim, hidden_dim = 784, 47, 100\n",
    "\n",
    "learning_rates = [0.0001, 0.001, 0.01, 0.015, 0.1, 0.2, 0.5]  # scale for random parameter initialisation\n",
    "final_errors_train = []\n",
    "final_errors_valid = []\n",
    "final_accs_train = []\n",
    "final_accs_valid = []\n",
    "\n",
    "for i, learning_rate in enumerate(learning_rates):\n",
    "\n",
    "    print('-' * 80)\n",
    "    print('learning_rate={0:.2e}'\n",
    "          .format(learning_rate))\n",
    "    print('-' * 80)\n",
    "    \n",
    "    # Reset random number generator and data provider states on each run\n",
    "    # to ensure reproducibility of results\n",
    "    rng.seed(seed)\n",
    "    train_data.reset()\n",
    "    valid_data.reset()\n",
    "\n",
    "    weights_init = GlorotUniformInit(rng=rng)\n",
    "    biases_init = ConstantInit(0.)\n",
    "\n",
    "    # Create a model with five hidden layers\n",
    "    model = MultipleLayerModel([\n",
    "        AffineLayer(input_dim, hidden_dim, weights_init, biases_init), \n",
    "        ReluLayer(),\n",
    "        AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "        ReluLayer(),\n",
    "        AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "        ReluLayer(),\n",
    "        AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "        ReluLayer(),\n",
    "        AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "        ReluLayer(),\n",
    "        AffineLayer(hidden_dim, output_dim, weights_init, biases_init)\n",
    "    ])\n",
    "\n",
    "    # Initialise a cross entropy error object\n",
    "    error = CrossEntropySoftmaxError()\n",
    "\n",
    "    # Use a basic gradient descent learning rule\n",
    "    learning_rule = GradientDescentLearningRule(learning_rate=learning_rate)\n",
    "\n",
    "    # Remember to use notebook=False when you write a script to be run in a terminal\n",
    "    stats, keys, run_time, fig_1, ax_1, fig_2, ax_2 = train_model_and_plot_stats(\n",
    "        model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval, fig_name='five_hidden_layers_' + str(i) + '.pdf', notebook=True)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    print('    final error(train) = {0:.2e}'.format(stats[-1, keys['error(train)']]))\n",
    "    print('    final error(valid) = {0:.2e}'.format(stats[-1, keys['error(valid)']]))\n",
    "    print('    final acc(train)   = {0:.2e}'.format(stats[-1, keys['acc(train)']]))\n",
    "    print('    final acc(valid)   = {0:.2e}'.format(stats[-1, keys['acc(valid)']]))\n",
    "    print('    run time per epoch = {0:.2f}'.format(run_time * 1. / num_epochs))\n",
    "\n",
    "    final_errors_train.append(stats[-1, keys['error(train)']])\n",
    "    final_errors_valid.append(stats[-1, keys['error(valid)']])\n",
    "    final_accs_train.append(stats[-1, keys['acc(train)']])\n",
    "    final_accs_valid.append(stats[-1, keys['acc(valid)']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 0\n",
    "print('| batch_size | learning_rate | final error(train) | final error(valid) | final acc(train) | final acc(valid) |')\n",
    "print('|------------|---------------|--------------------|--------------------|------------------|------------------|')\n",
    "for learning_rate in learning_rates:\n",
    "    print('| {0:f}        | {1:.2e}        | {2:.2e}           | {3:.2e}           |  {4:.2f}            | {5:.2f}             |'\n",
    "          .format(batch_size, learning_rate, \n",
    "                  final_errors_train[j], final_errors_valid[j],\n",
    "                  final_accs_train[j], final_accs_valid[j]))\n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### batch_size = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set batch size\n",
    "batch_size = 200\n",
    "\n",
    "# Create data provider objects for the MNIST data set\n",
    "train_data = EMNISTDataProvider('train', batch_size=batch_size, rng=rng)\n",
    "valid_data = EMNISTDataProvider('valid', batch_size=batch_size, rng=rng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup hyperparameters\n",
    "num_epochs = 150\n",
    "stats_interval = 1\n",
    "input_dim, output_dim, hidden_dim = 784, 47, 100\n",
    "\n",
    "learning_rates = [0.0001, 0.001, 0.01, 0.015, 0.1, 0.2, 0.5]  # scale for random parameter initialisation\n",
    "final_errors_train = []\n",
    "final_errors_valid = []\n",
    "final_accs_train = []\n",
    "final_accs_valid = []\n",
    "\n",
    "for i, learning_rate in enumerate(learning_rates):\n",
    "\n",
    "    print('-' * 80)\n",
    "    print('learning_rate={0:.2e}'\n",
    "          .format(learning_rate))\n",
    "    print('-' * 80)\n",
    "    \n",
    "    # Reset random number generator and data provider states on each run\n",
    "    # to ensure reproducibility of results\n",
    "    rng.seed(seed)\n",
    "    train_data.reset()\n",
    "    valid_data.reset()\n",
    "\n",
    "    weights_init = GlorotUniformInit(rng=rng)\n",
    "    biases_init = ConstantInit(0.)\n",
    "\n",
    "    # Create a model with two hidden layers\n",
    "    model = MultipleLayerModel([\n",
    "        AffineLayer(input_dim, hidden_dim, weights_init, biases_init), \n",
    "        ReluLayer(),\n",
    "        AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "        ReluLayer(),\n",
    "        AffineLayer(hidden_dim, output_dim, weights_init, biases_init)\n",
    "    ])\n",
    "\n",
    "    # Initialise a cross entropy error object\n",
    "    error = CrossEntropySoftmaxError()\n",
    "\n",
    "    # Use a basic gradient descent learning rule\n",
    "    learning_rule = GradientDescentLearningRule(learning_rate=learning_rate)\n",
    "\n",
    "    # Remember to use notebook=False when you write a script to be run in a terminal\n",
    "    stats, keys, run_time, fig_1, ax_1, fig_2, ax_2 = train_model_and_plot_stats(\n",
    "        model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval, fig_name='2_two_hidden_layers_' + str(i) + '.pdf', notebook=True)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    print('    final error(train) = {0:.2e}'.format(stats[-1, keys['error(train)']]))\n",
    "    print('    final error(valid) = {0:.2e}'.format(stats[-1, keys['error(valid)']]))\n",
    "    print('    final acc(train)   = {0:.2e}'.format(stats[-1, keys['acc(train)']]))\n",
    "    print('    final acc(valid)   = {0:.2e}'.format(stats[-1, keys['acc(valid)']]))\n",
    "    print('    run time per epoch = {0:.2f}'.format(run_time * 1. / num_epochs))\n",
    "\n",
    "    final_errors_train.append(stats[-1, keys['error(train)']])\n",
    "    final_errors_valid.append(stats[-1, keys['error(valid)']])\n",
    "    final_accs_train.append(stats[-1, keys['acc(train)']])\n",
    "    final_accs_valid.append(stats[-1, keys['acc(valid)']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 0\n",
    "print('| batch_size | learning_rate | final error(train) | final error(valid) | final acc(train) | final acc(valid) |')\n",
    "print('|------------|---------------|--------------------|--------------------|------------------|------------------|')\n",
    "for learning_rate in learning_rates:\n",
    "    print('| {0:f}        | {1:.2e}        | {2:.2e}           | {3:.2e}           |  {4:.2f}            | {5:.2f}             |'\n",
    "          .format(batch_size, learning_rate, \n",
    "                  final_errors_train[j], final_errors_valid[j],\n",
    "                  final_accs_train[j], final_accs_valid[j]))\n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup hyperparameters\n",
    "num_epochs = 150\n",
    "stats_interval = 1\n",
    "input_dim, output_dim, hidden_dim = 784, 47, 100\n",
    "\n",
    "learning_rates = [0.0001, 0.001, 0.01, 0.015, 0.1, 0.2, 0.5]  # scale for random parameter initialisation\n",
    "final_errors_train = []\n",
    "final_errors_valid = []\n",
    "final_accs_train = []\n",
    "final_accs_valid = []\n",
    "\n",
    "for i, learning_rate in enumerate(learning_rates):\n",
    "\n",
    "    print('-' * 80)\n",
    "    print('learning_rate={0:.2e}'\n",
    "          .format(learning_rate))\n",
    "    print('-' * 80)\n",
    "    \n",
    "    # Reset random number generator and data provider states on each run\n",
    "    # to ensure reproducibility of results\n",
    "    rng.seed(seed)\n",
    "    train_data.reset()\n",
    "    valid_data.reset()\n",
    "\n",
    "    weights_init = GlorotUniformInit(rng=rng)\n",
    "    biases_init = ConstantInit(0.)\n",
    "\n",
    "    # Create a model with three hidden layers\n",
    "    model = MultipleLayerModel([\n",
    "        AffineLayer(input_dim, hidden_dim, weights_init, biases_init), \n",
    "        ReluLayer(),\n",
    "        AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "        ReluLayer(),\n",
    "        AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "        ReluLayer(),\n",
    "        AffineLayer(hidden_dim, output_dim, weights_init, biases_init)\n",
    "    ])\n",
    "\n",
    "    # Initialise a cross entropy error object\n",
    "    error = CrossEntropySoftmaxError()\n",
    "\n",
    "    # Use a basic gradient descent learning rule\n",
    "    learning_rule = GradientDescentLearningRule(learning_rate=learning_rate)\n",
    "\n",
    "    # Remember to use notebook=False when you write a script to be run in a terminal\n",
    "    stats, keys, run_time, fig_1, ax_1, fig_2, ax_2 = train_model_and_plot_stats(\n",
    "        model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval, fig_name='2_three_hidden_layers_' + str(i) + '.pdf', notebook=True)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    print('    final error(train) = {0:.2e}'.format(stats[-1, keys['error(train)']]))\n",
    "    print('    final error(valid) = {0:.2e}'.format(stats[-1, keys['error(valid)']]))\n",
    "    print('    final acc(train)   = {0:.2e}'.format(stats[-1, keys['acc(train)']]))\n",
    "    print('    final acc(valid)   = {0:.2e}'.format(stats[-1, keys['acc(valid)']]))\n",
    "    print('    run time per epoch = {0:.2f}'.format(run_time * 1. / num_epochs))\n",
    "\n",
    "    final_errors_train.append(stats[-1, keys['error(train)']])\n",
    "    final_errors_valid.append(stats[-1, keys['error(valid)']])\n",
    "    final_accs_train.append(stats[-1, keys['acc(train)']])\n",
    "    final_accs_valid.append(stats[-1, keys['acc(valid)']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 0\n",
    "print('| batch_size | learning_rate | final error(train) | final error(valid) | final acc(train) | final acc(valid) |')\n",
    "print('|------------|---------------|--------------------|--------------------|------------------|------------------|')\n",
    "for learning_rate in learning_rates:\n",
    "    print('| {0:f}        | {1:.2e}        | {2:.2e}           | {3:.2e}           |  {4:.2f}            | {5:.2f}             |'\n",
    "          .format(batch_size, learning_rate, \n",
    "                  final_errors_train[j], final_errors_valid[j],\n",
    "                  final_accs_train[j], final_accs_valid[j]))\n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4 layers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup hyperparameters\n",
    "num_epochs = 150\n",
    "stats_interval = 1\n",
    "input_dim, output_dim, hidden_dim = 784, 47, 100\n",
    "\n",
    "learning_rates = [0.0001, 0.001, 0.01, 0.015, 0.1, 0.2, 0.5]  # scale for random parameter initialisation\n",
    "final_errors_train = []\n",
    "final_errors_valid = []\n",
    "final_accs_train = []\n",
    "final_accs_valid = []\n",
    "\n",
    "for i, learning_rate in enumerate(learning_rates):\n",
    "\n",
    "    print('-' * 80)\n",
    "    print('learning_rate={0:.2e}'\n",
    "          .format(learning_rate))\n",
    "    print('-' * 80)\n",
    "    \n",
    "    # Reset random number generator and data provider states on each run\n",
    "    # to ensure reproducibility of results\n",
    "    rng.seed(seed)\n",
    "    train_data.reset()\n",
    "    valid_data.reset()\n",
    "\n",
    "    weights_init = GlorotUniformInit(rng=rng)\n",
    "    biases_init = ConstantInit(0.)\n",
    "\n",
    "    # Create a model with four hidden layers\n",
    "    model = MultipleLayerModel([\n",
    "        AffineLayer(input_dim, hidden_dim, weights_init, biases_init), \n",
    "        ReluLayer(),\n",
    "        AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "        ReluLayer(),\n",
    "        AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "        ReluLayer(),\n",
    "        AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "        ReluLayer(),\n",
    "        AffineLayer(hidden_dim, output_dim, weights_init, biases_init)\n",
    "    ])\n",
    "\n",
    "    # Initialise a cross entropy error object\n",
    "    error = CrossEntropySoftmaxError()\n",
    "\n",
    "    # Use a basic gradient descent learning rule\n",
    "    learning_rule = GradientDescentLearningRule(learning_rate=learning_rate)\n",
    "\n",
    "    # Remember to use notebook=False when you write a script to be run in a terminal\n",
    "    stats, keys, run_time, fig_1, ax_1, fig_2, ax_2 = train_model_and_plot_stats(\n",
    "        model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval, fig_name='2_four_hidden_layers_' + str(i) + '.pdf', notebook=True)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    print('    final error(train) = {0:.2e}'.format(stats[-1, keys['error(train)']]))\n",
    "    print('    final error(valid) = {0:.2e}'.format(stats[-1, keys['error(valid)']]))\n",
    "    print('    final acc(train)   = {0:.2e}'.format(stats[-1, keys['acc(train)']]))\n",
    "    print('    final acc(valid)   = {0:.2e}'.format(stats[-1, keys['acc(valid)']]))\n",
    "    print('    run time per epoch = {0:.2f}'.format(run_time * 1. / num_epochs))\n",
    "\n",
    "    final_errors_train.append(stats[-1, keys['error(train)']])\n",
    "    final_errors_valid.append(stats[-1, keys['error(valid)']])\n",
    "    final_accs_train.append(stats[-1, keys['acc(train)']])\n",
    "    final_accs_valid.append(stats[-1, keys['acc(valid)']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 0\n",
    "print('| batch_size | learning_rate | final error(train) | final error(valid) | final acc(train) | final acc(valid) |')\n",
    "print('|------------|---------------|--------------------|--------------------|------------------|------------------|')\n",
    "for learning_rate in learning_rates:\n",
    "    print('| {0:f}        | {1:.2e}        | {2:.2e}           | {3:.2e}           |  {4:.2f}            | {5:.2f}             |'\n",
    "          .format(batch_size, learning_rate, \n",
    "                  final_errors_train[j], final_errors_valid[j],\n",
    "                  final_accs_train[j], final_accs_valid[j]))\n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup hyperparameters\n",
    "num_epochs = 150\n",
    "stats_interval = 1\n",
    "input_dim, output_dim, hidden_dim = 784, 47, 100\n",
    "\n",
    "learning_rates = [0.0001, 0.001, 0.01, 0.015, 0.1, 0.2, 0.5]  # scale for random parameter initialisation\n",
    "final_errors_train = []\n",
    "final_errors_valid = []\n",
    "final_accs_train = []\n",
    "final_accs_valid = []\n",
    "\n",
    "for i, learning_rate in enumerate(learning_rates):\n",
    "\n",
    "    print('-' * 80)\n",
    "    print('learning_rate={0:.2e}'\n",
    "          .format(learning_rate))\n",
    "    print('-' * 80)\n",
    "    \n",
    "    # Reset random number generator and data provider states on each run\n",
    "    # to ensure reproducibility of results\n",
    "    rng.seed(seed)\n",
    "    train_data.reset()\n",
    "    valid_data.reset()\n",
    "\n",
    "    weights_init = GlorotUniformInit(rng=rng)\n",
    "    biases_init = ConstantInit(0.)\n",
    "\n",
    "    # Create a model with five hidden layers\n",
    "    model = MultipleLayerModel([\n",
    "        AffineLayer(input_dim, hidden_dim, weights_init, biases_init), \n",
    "        ReluLayer(),\n",
    "        AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "        ReluLayer(),\n",
    "        AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "        ReluLayer(),\n",
    "        AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "        ReluLayer(),\n",
    "        AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "        ReluLayer(),\n",
    "        AffineLayer(hidden_dim, output_dim, weights_init, biases_init)\n",
    "    ])\n",
    "\n",
    "    # Initialise a cross entropy error object\n",
    "    error = CrossEntropySoftmaxError()\n",
    "\n",
    "    # Use a basic gradient descent learning rule\n",
    "    learning_rule = GradientDescentLearningRule(learning_rate=learning_rate)\n",
    "\n",
    "    # Remember to use notebook=False when you write a script to be run in a terminal\n",
    "    stats, keys, run_time, fig_1, ax_1, fig_2, ax_2 = train_model_and_plot_stats(\n",
    "        model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval, fig_name='2_five_hidden_layers_' + str(i) + '.pdf', notebook=True)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    print('    final error(train) = {0:.2e}'.format(stats[-1, keys['error(train)']]))\n",
    "    print('    final error(valid) = {0:.2e}'.format(stats[-1, keys['error(valid)']]))\n",
    "    print('    final acc(train)   = {0:.2e}'.format(stats[-1, keys['acc(train)']]))\n",
    "    print('    final acc(valid)   = {0:.2e}'.format(stats[-1, keys['acc(valid)']]))\n",
    "    print('    run time per epoch = {0:.2f}'.format(run_time * 1. / num_epochs))\n",
    "\n",
    "    final_errors_train.append(stats[-1, keys['error(train)']])\n",
    "    final_errors_valid.append(stats[-1, keys['error(valid)']])\n",
    "    final_accs_train.append(stats[-1, keys['acc(train)']])\n",
    "    final_accs_valid.append(stats[-1, keys['acc(valid)']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 0\n",
    "print('| batch_size | learning_rate | final error(train) | final error(valid) | final acc(train) | final acc(valid) |')\n",
    "print('|------------|---------------|--------------------|--------------------|------------------|------------------|')\n",
    "for learning_rate in learning_rates:\n",
    "    print('| {0:f}        | {1:.2e}        | {2:.2e}           | {3:.2e}           |  {4:.2f}            | {5:.2f}             |'\n",
    "          .format(batch_size, learning_rate, \n",
    "                  final_errors_train[j], final_errors_valid[j],\n",
    "                  final_accs_train[j], final_accs_valid[j]))\n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from itertools import product\n",
    "a = np.array([1, 2, 3, 4, 5 , 6])\n",
    "b = np.array(['a', 'b', 'c'])\n",
    "\n",
    "for element in product(a, b):\n",
    "    print(element[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlp.initialisers import UniformInit\n",
    "init_scale = 0.01\n",
    "weights_init = UniformInit(-init_scale, init_scale, rng=rng)\n",
    "print (weights_init)\n",
    "print (rng.uniform(-1, 1, 5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
